{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Took finedtuned hyperparameter from a notebook where author gridsearch the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 80.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "target = pd.read_csv('./dataset_final/target.csv')\n",
    "train = pd.read_csv('./dataset_final/train.csv')\n",
    "test = pd.read_csv('./dataset_final/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = pd.read_csv('./dataset_final/test2.csv')\n",
    "test_id = test_id['TransactionID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:39:32] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { bagging_fraction, feature_fraction, min_child_samples, num_leaves } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:39:42] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 58min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(bagging_fraction=0.8993155305338455, base_score=0.5,\n",
       "              booster='gbtree', colsample_bylevel=1, colsample_bynode=1,\n",
       "              colsample_bytree=0.7463058454739352,\n",
       "              feature_fraction=0.7989765808988153, gamma=0.6665437467229817,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.013887824598276186, max_delta_step=0,\n",
       "              max_depth=16, min_child_samples=170, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=300,\n",
       "              n_jobs=4, num_leaves=220, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0.39871702770778467, reg_lambda=0.24309304355829786,\n",
       "              scale_pos_weight=1, subsample=0.7, tree_method='exact',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# XGBOOST\n",
    "xgmodel = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    bagging_fraction = 0.8993155305338455,\n",
    "    colsample_bytree = 0.7463058454739352,\n",
    "    feature_fraction = 0.7989765808988153,\n",
    "    gamma = 0.6665437467229817,\n",
    "    learning_rate = 0.013887824598276186,\n",
    "    max_depth = 16,\n",
    "    min_child_samples = 170,\n",
    "    num_leaves = 220,\n",
    "    reg_alpha = 0.39871702770778467,\n",
    "    reg_lambda = 0.24309304355829786,\n",
    "    subsample = 0.7\n",
    ")\n",
    "\n",
    "xgmodel.fit(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xg = xgmodel.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['TransactionID'] = test_id\n",
    "sub['isFraud'] = y_pred_xg[:, 1]\n",
    "sub.to_csv('submission_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score was: 0.924261"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
